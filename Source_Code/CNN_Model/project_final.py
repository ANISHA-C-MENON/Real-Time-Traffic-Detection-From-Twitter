# -*- coding: utf-8 -*-
"""project_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DfZs855pVB48oUpfB7emR3A8U2MbEkvW
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import tensorflow as tf
import pydot
import re
import string
import itertools
import gensim

from gensim.utils import tokenize
from gensim.parsing.preprocessing import STOPWORDS
from tensorflow.keras import regularizers
from tensorflow.keras import regularizers
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import preprocessing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import *
from keras.optimizers import SGD

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix

from google.colab import drive

from collections import Counter

#preprocessing(removel of emoji, punctuation, urls, stop words)

def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_punct(text):
    text_nopunct = ''
    text_nopunct = re.sub('['+string.punctuation+']', '', text)
    return text_nopunct

def remove_url(text): 
    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    return url_pattern.sub(r'', text)

def lower_token(tokens): 
    return [w.lower() for w in tokens]

all_stopwords_gensim = STOPWORDS
def removeStopWords(tokens): 
    return [word for word in tokens if word not in all_stopwords_gensim and word.isalpha()]

#loading dataset

drive.mount("/content/gdrive")
train =pd.read_csv('/content/gdrive/My Drive/train_test_merged_dataset_class2_with_location.csv')
df1 = pd.DataFrame(train)

#creating dataframe

train.columns = ['Label','ID','Text','Location']
df1 = df1.sample(frac = 1)
train = df1
train['Text'] = train['Text'].apply(remove_emoji)
train['Text'] = train['Text'].apply(remove_url)
train['Text'] = train['Text'].apply(remove_punct)

df1.head(10)

#tokenization

tokens_train = [list(tokenize(sen)) for sen in train.Text]
print(tokens_train[:10])

lower_tokens_train = [lower_token(token) for token in tokens_train]
print(lower_tokens_train[:10])

filtered_words_train = [removeStopWords(sen) for sen in lower_tokens_train]
train['Text_Final'] = [' '.join(sen) for sen in filtered_words_train]
train['tokens'] = filtered_words_train
print(filtered_words_train[:10])

train = train[['Text_Final', 'tokens', 'Label','Location']]
print(train)

num_words = 20000
 
tokenizer = Tokenizer(num_words=num_words,oov_token="unk")
tokenizer.fit_on_texts(train['Text_Final'].tolist())

word_index = tokenizer.word_index
print(tokenizer.word_index)
print('Found %s unique tokens.' % len(word_index))

#train-test split(ration 0.2)

data_train, data_valid, label_train, label_valid = train_test_split(train['tokens'].tolist(),\
                                                      train['Label'].tolist(),\
                                                      test_size=0.2,\
                                                      stratify = train['Label'].tolist(),\
                                                      random_state=0)

print('Train data len:'+str(len(data_train)))
print('Class distribution'+str(Counter(label_train)))
print('Valid data len:'+str(len(data_valid)))
print('Class distribution'+ str(Counter(label_valid)))

#build training vocabulary and get maximum training sentence length and total number of words training data

all_training_words = [word for tokens in data_train for word in tokens]
training_sentence_lengths = [len(tokens) for tokens in data_train]
TRAINING_VOCAB = sorted(list(set(all_training_words)))

print("%s words total, with a vocabulary size of %s" % (len(all_training_words), len(TRAINING_VOCAB)))
max_train_len=max(training_sentence_lengths)
print("Max sentence length is %s" % max_train_len)

#build validation vocabulary and get maximum sentence length and total number of words in validation data
all_valid_words = [word for tokens in data_valid for word in tokens]
valid_sentence_lengths = [len(tokens) for tokens in data_valid]
VALID_VOCAB = sorted(list(set(all_valid_words)))

print("%s words total, with a vocabulary size of %s" % (len(all_valid_words), len(VALID_VOCAB)))
max_valid_len=max(valid_sentence_lengths)
print("Max sentence length is %s" % max_valid_len)

x_train = np.array( tokenizer.texts_to_sequences(data_train) )
x_valid = np.array( tokenizer.texts_to_sequences(data_valid) )
max_len_total=max(max_train_len,max_valid_len)

#padding(post padding)

x_train = pad_sequences(x_train, padding='post', maxlen=max_len_total)
x_valid = pad_sequences(x_valid, padding='post', maxlen=max_len_total)

print(x_train[0])

train_ds = tf.data.Dataset.from_tensor_slices((x_train,label_train))
valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,label_valid))

#Embedding word2vec

import gensim
from gensim import models
import gensim.downloader as api
 
wv = api.load('word2vec-google-news-300')
 
vec_traffic = wv['traffic']
print(vec_traffic.shape)
print(wv.most_similar('traffic'))

unique_words = len(VALID_VOCAB)+len(TRAINING_VOCAB)
total_words = unique_words + 1
skipped_words = 0
embedding_dim = 300 
embedding_matrix = np.zeros((total_words, embedding_dim))
for word, index in tokenizer.word_index.items():
    try:
        embedding_vector = wv[word]
    except:
        skipped_words = skipped_words+1
        pass
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
print("Embeddings Matrix shape : ",embedding_matrix[2])

max_length=max_len_total

embedding_layer = Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)

#adding layers(5 layers)

model = tf.keras.Sequential()
model.add(embedding_layer)                                   

model.add(tf.keras.layers.Conv1D(128,2, activation='relu',\
                                 kernel_regularizer = regularizers.l2(0.0005),\
                                 bias_regularizer = regularizers.l2(0.0005)))                               


model.add(tf.keras.layers.GlobalMaxPooling1D())

model.add(tf.keras.layers.Dropout(0.3))

model.add(tf.keras.layers.Dense(2, activation='sigmoid',\
                                kernel_regularizer=regularizers.l2(0.005),\
                                bias_regularizer=regularizers.l2(0.005),))
                               
model.summary()
opt = SGD(lr=0.01, momentum=0.9)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

# Fit the model using the train and test datasets.
#history = model.fit(x_train, train_labels,validation_data= (x_test,test_labels),epochs=epochs )

x_train=np.array(x_train)
label_train=np.array(label_train)
x1_valid=np.array(x_valid)
label1_valid=np.array(label_valid)
l_train = to_categorical(label_train, 2)
l_valid = to_categorical(label1_valid, 2)
history = model.fit(x_train,l_train,
                    shuffle=False,
                    validation_data=(x1_valid, l_valid),
                    batch_size=64,
                    epochs=20,
                    verbose=1)
#saving model
model.save("model.h5")

#loss graph
plt.plot(history.history['loss'], label=' training data')
plt.plot(history.history['val_loss'], label='validation data')
plt.xticks(np.linspace(0, 20, 11))
plt.title('Loss for Text Classification')
plt.ylabel('Loss value')
plt.xlabel('No. epoch')
plt.legend(loc="upper right")
plt.show()

#accuracy graph
plt.plot(history.history['val_accuracy'], label='validation data')
plt.xticks(np.linspace(0, 20, 11))
plt.title('Accuracy for Text Classification')
plt.ylabel('Accuracy value')
plt.xlabel('No. epoch')
plt.legend(loc="lower right")
plt.show()

y_pred_keras = model.predict_proba(x_valid)
y_pred_keras = y_pred_keras[:, 1]

print(y_pred_keras)

s=label1_valid
print(s)

#roc curve
from sklearn import metrics
fpr, tpr, thresholds = roc_curve(s, y_pred_keras)
roc_auc = metrics.auc(fpr, tpr)

# calculate the g-mean for each threshold
gmeans = np.sqrt(tpr * (1-fpr))

# locate the index of the largest g-mean
ix = np.argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
print('AUC=%f'% (roc_auc))

# plot the roc curve for the model
plt.plot([0,1], [0,1], linestyle='--', label='No Skill')
plt.plot(fpr, tpr, marker='.', label='cnn')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')

# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()

# show the plot
plt.show()

#confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

cnf_matrix = confusion_matrix(s, y_pred_keras.round())
np.set_printoptions(precision=2)

plt.figure()
plot_confusion_matrix(cnf_matrix, classes=['positive','negative'],
                      title='Confusion matrix, without normalization')

tn, fp, fn, tp = confusion_matrix(s, y_pred_keras.round()).ravel()
print(y_pred_keras.round())
print("True Negatives: ",tn)
print("False Positives: ",fp)
print("False Negatives: ",fn)
print("True Positives: ",tp)

#reloading the model
new_model = tf.keras.models.load_model('/content/gdrive/My Drive/model.h5')
new_model.summary()

#prediction
print("Generate predictions for all samples")
y_pred = (model.predict_proba(x_valid) >= 0.467496).astype(int)
y_pred = y_pred.argmax(axis=1)
print(y_pred)

#labeling
y_pred = np.where((y_pred == 0),'negative',y_pred)
y_pred = np.where((y_pred == '1'),'positive',y_pred)
s = np.where((s == '0'),'negative',s)
s = np.where((s == '1'),'positive',s)
print(y_pred)
print(s)

y_test = s.tolist()
y_pred = y_pred.tolist()

#classification report
labels=['negative','positive']
print(classification_report(y_test,y_pred,
                            labels=labels))
